{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import math\n",
    "import hashlib, binascii\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "from shutil import copyfile\n",
    "from os import listdir\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFiles():\n",
    "    dlFiles = listdir(\"./download\")\n",
    "    try:\n",
    "        dlFiles.remove('.DS_Store')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    result = []\n",
    "    for file in dlFiles:\n",
    "        fileObject={\"name\":None, \"data\":None}\n",
    "        newData = pd.read_csv(\"./download/\"+file, header=None, names=newColumns)\n",
    "        fileObject[\"name\"] = file.split(\".\")[0]\n",
    "        fileObject[\"data\"] = newData\n",
    "        result.append(fileObject)\n",
    "    return result\n",
    "\n",
    "\n",
    "def getFile(file):\n",
    "    if file == \"data\":\n",
    "        return pd.read_csv(\"./data/data.csv\", header=None, names=oldColumns,index_col=False)\n",
    "    elif file == \"processed\":\n",
    "        return pd.read_csv(\"./data/processed.csv\",index_col=False)\n",
    "    elif file == \"maps\":\n",
    "        return pd.read_csv(\"./data/1to1maps.csv\", header=None, names=['item', 'subCategory'])\n",
    "    elif file == \"subCategories\":\n",
    "        return pd.read_csv(\"./data/categories.csv\", header=None, names=['item', 'subCategory'])\n",
    "    elif file == \"categories\":\n",
    "        return pd.read_csv(\"./data/breakdown.csv\", header=None, names=['subCategory', 'category'])\n",
    "    \n",
    "def writeFile(file, df):\n",
    "    if file ==\"maps\":\n",
    "        df.to_csv('./data/1to1maps.csv', index=False, header=False)  \n",
    "    elif file==\"subCategories\":\n",
    "        df.to_csv('./data/categories.csv', index=False, header=False)  \n",
    "    elif file ==\"data\":\n",
    "        df.to_csv('./data/data.csv', index=False, header=False)  \n",
    "\n",
    "        \n",
    "\n",
    "def saveDf(df, fileName, path, header = False):\n",
    "    miliTime = int(round(time.time()))\n",
    "    readableTime = datetime.utcfromtimestamp(miliTime).strftime('%Y-%m-%d')\n",
    "    df.to_csv(f'./backup/{fileName}-{readableTime}.csv', index=False, header=header)\n",
    "    df.to_csv(f'./{path}/{fileName}.csv', index=False, header=header)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hash Data\n",
    "\n",
    "oldColumns=['date','item','debit','credit','subCategory','hash', 'account']\n",
    "addedColumns = ['date','item','debit','credit','hash','account']\n",
    "newColumns=['date','item','debit','credit','card']\n",
    "processedColumns=['item','category','subCategory','date','year','month','debit','credit','balance', 'account']\n",
    "\n",
    "def hashit(df):\n",
    "    hashs = []\n",
    "    for index, row in df.iterrows():\n",
    "        h = hashlib.new('ripemd160')\n",
    "        it = str(row['item'])\n",
    "        it2= ' '.join(re.findall(r\"[\\w']+\", it))\n",
    "        h.update(it2.encode())\n",
    "        h.update(str(row['credit']).encode())\n",
    "        h.update(str(row['debit']).encode())\n",
    "        h.update(str(row['date']).encode())\n",
    "        hashed =h.hexdigest()\n",
    "        hashs.append(hashed)\n",
    "    return hashs\n",
    "\n",
    "def testDf(df):\n",
    "    assert df.dtypes['debit'] == 'float64'\n",
    "    assert df.dtypes['credit'] == 'float64'\n",
    "\n",
    "def fixDf(df):\n",
    "    global oldColumns\n",
    "    df['subCategory'].fillna(\"\",inplace=True)\n",
    "    df=df[oldColumns]\n",
    "    return df\n",
    "\n",
    "def findNewItems(old, new, fileName):\n",
    "    new['hash']= hashit(new)\n",
    "    hashfound = []\n",
    "    for index, row in new.iterrows():\n",
    "        hashfound.append(row['hash'] in old['hash'].values)\n",
    "    new['hashfound']=hashfound\n",
    "    new.loc[new['hashfound'] == False, 'account'] = fileName;\n",
    "    newItems = new[new['hashfound'] == False]\n",
    "    return newItems\n",
    "\n",
    "def convertToJsonArray(df):\n",
    "    columns = df.columns\n",
    "    result = []\n",
    "    for i, row in df.iterrows():\n",
    "        dummy = {}\n",
    "        for column in columns:\n",
    "            dummy[column]=row[column]\n",
    "        result.append(dummy)\n",
    "    return(result)\n",
    "\n",
    "\n",
    "def writeToJson(df):\n",
    "    items = convertToJsonArray(df)\n",
    "    with open('../analysis/js/data.json', 'w') as jsonFile:\n",
    "        json.dump(items, jsonFile)\n",
    "\n",
    "def listNewItems(files):\n",
    "    global oldColumns\n",
    "    old = getFile('data')\n",
    "    old['subCategory'].fillna(\"\",inplace=True)\n",
    "    old.fillna(value=0,inplace=True)\n",
    "    testDf(old)\n",
    "\n",
    "    combinedAll = old[oldColumns]\n",
    "    combinedNew = pd.DataFrame(columns=oldColumns)\n",
    "\n",
    "    for new in files:\n",
    "        newData = new['data']\n",
    "        newName = new['name']\n",
    "        newData.fillna(value=0,inplace=True)\n",
    "        testDf(newData)\n",
    "\n",
    "        newItems = findNewItems(combinedAll, newData, newName)\n",
    "        newToSave = newItems[addedColumns]\n",
    "\n",
    "        print(f\"{newName} - {len(newToSave)} new items found\")\n",
    "\n",
    "        combinedAll = pd.concat([combinedAll, newToSave])\n",
    "        combinedNew = pd.concat([combinedNew, newToSave])\n",
    "        \n",
    "    combinedNew = fixDf(combinedNew)\n",
    "    return combinedNew\n",
    "\n",
    "# Process Hashed Data\n",
    "def processData(newItems,doAll = False):\n",
    "    if doAll:\n",
    "        data = getFile('data')\n",
    "    else:\n",
    "        newItems.reset_index(inplace=True)\n",
    "        data = newItems.copy()\n",
    "\n",
    "    maps = getFile('maps')\n",
    "    subCategories = getFile('subCategories')\n",
    "    categories = getFile('categories')\n",
    "\n",
    "    categoryMap ={}\n",
    "    for i, row in categories.iterrows():\n",
    "        categoryMap[row['subCategory']] = row['category']\n",
    "\n",
    "    data.fillna(\"\", inplace=True)\n",
    "\n",
    "    subCatArray = []\n",
    "    # first mapping the 1to1 mappings\n",
    "    for i, row in data.iterrows():\n",
    "        if (row['subCategory'] != \"\"):\n",
    "            subCatArray.append(row['subCategory'])\n",
    "        else:\n",
    "            try:\n",
    "                index = pd.Index(maps['item']).get_loc(row['item'].rstrip())\n",
    "                subCategory = maps.loc[index]['subCategory']\n",
    "                subCatArray.append(subCategory)\n",
    "            except:\n",
    "                subCatArray.append(\"\")\n",
    "\n",
    "    # then mapping all the general categories\n",
    "    data['subCategory'] = subCatArray\n",
    "    subCatArray = pd.Series(subCatArray) \n",
    "\n",
    "\n",
    "    for i, categoryRow in subCategories.iloc[::-1].iterrows():\n",
    "        indo = ((data['item'].str.contains(categoryRow['item'])) & (data['subCategory']==\"\"))\n",
    "        subCatArray[indo] = categoryRow['subCategory']\n",
    "\n",
    "    data['balance']=data['credit']-data['debit']\n",
    "\n",
    "    # finally taking care of special categories with logic\n",
    "    specialCategories = subCategories[subCategories['item'].str.contains('{{')]\n",
    "    for i, categoryRow in specialCategories.iterrows():\n",
    "        itemValuePair = categoryRow['item'].replace('}}', '').split('{{')\n",
    "        indo = (data['item'].str.contains(itemValuePair[0].rstrip()) & (data['balance']==(float(itemValuePair[1]))))\n",
    "        subCatArray[indo] = categoryRow['subCategory']\n",
    "\n",
    "    data['subCategory'] = subCatArray\n",
    "    data['category'] = data['subCategory'].map(categoryMap)\n",
    "    data['year']= pd.to_datetime(data['date']).dt.year\n",
    "    data['month']= pd.to_datetime(data['date']).dt.month    \n",
    "    return data\n",
    "\n",
    "def runProcess(files):\n",
    "    newItems = listNewItems(files)\n",
    "    if(newItems['item'].count() > 0):\n",
    "        processedData = processData(newItems)   \n",
    "        dataWithoutCategory = (processedData[processedData['subCategory'] == \"\"])\n",
    "        if(len(dataWithoutCategory) == 0):\n",
    "            processedAlready = getFile('processed')\n",
    "            processedAll = pd.concat([processedData, processedAlready])\n",
    "            processedToSave = processedAll[processedColumns].sort_values(by='date', ascending=False)\n",
    "            saveDf(processedToSave, 'processed', 'data', True)\n",
    "\n",
    "            dataAll = getFile('data')\n",
    "            combinedData = pd.concat([dataAll, newItems])\n",
    "            combinedData = combinedData[oldColumns]\n",
    "            saveDf(combinedData, 'data', 'data', False)\n",
    "\n",
    "            writeToJson(processedToSave)\n",
    "\n",
    "            print(\"SAVED\")\n",
    "        else:\n",
    "            print('Found Gaps, NOT SAVED')\n",
    "            print(dataWithoutCategory[['item','date','balance']])\n",
    "    #       dataWithoutCategory[['item','date','balance']].to_csv('./processed/not_found.csv')\n",
    "    else:\n",
    "        print('no new items')\n",
    "        \n",
    "def resetToCurrentData():\n",
    "    processedData = processData(None, True)  \n",
    "    processedToSave = processedData[processedColumns].sort_values(by='date', ascending=False)\n",
    "    writeToJson(processedToSave)\n",
    "    saveDf(processedToSave, 'processed', 'data', True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # files should have schema [{'name':string, 'data':pd.DataFrame}, ...]\n",
    "files = getFiles()\n",
    "# runProcess(files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>item</th>\n",
       "      <th>debit</th>\n",
       "      <th>credit</th>\n",
       "      <th>card</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-02-01</td>\n",
       "      <td>Electronic Funds Transfer PREAUTHORIZED DEBIT ...</td>\n",
       "      <td>0.04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-31</td>\n",
       "      <td>Branch Transaction SERVICE CHARGE DISCOUNT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.45</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-31</td>\n",
       "      <td>Branch Transaction SERVICE CHARGE</td>\n",
       "      <td>7.45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-25</td>\n",
       "      <td>Internet Banking INTERNET BILL PAY 00000021643...</td>\n",
       "      <td>433.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-25</td>\n",
       "      <td>Electronic Funds Transfer PAY DTCG DELOITTE INC.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2634.50</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2019-01-23</td>\n",
       "      <td>Electronic Funds Transfer PREAUTHORIZED DEBIT ...</td>\n",
       "      <td>3258.39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2019-01-21</td>\n",
       "      <td>Electronic Funds Transfer PREAUTHORIZED DEBIT ...</td>\n",
       "      <td>35.37</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2019-01-21</td>\n",
       "      <td>Electronic Funds Transfer PREAUTHORIZED DEBIT ...</td>\n",
       "      <td>71.64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2019-01-21</td>\n",
       "      <td>Point of Sale - Interac RETAIL PURCHASE 902012...</td>\n",
       "      <td>27.73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2019-01-17</td>\n",
       "      <td>Internet Banking E-TRANSFER 101253831721 Hedie...</td>\n",
       "      <td>225.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2019-01-16</td>\n",
       "      <td>Electronic Funds Transfer PREAUTHORIZED DEBIT ...</td>\n",
       "      <td>630.22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2019-01-16</td>\n",
       "      <td>Point of Sale - Interac RETAIL PURCHASE 000068...</td>\n",
       "      <td>0.78</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2019-01-15</td>\n",
       "      <td>Electronic Funds Transfer DEPOSIT 0000759492 D...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>951.51</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2019-01-11</td>\n",
       "      <td>Electronic Funds Transfer PAY DTCG DELOITTE INC.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2634.49</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2019-01-07</td>\n",
       "      <td>Electronic Funds Transfer SAFETY DEPOSIT BOX R...</td>\n",
       "      <td>84.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2019-01-07</td>\n",
       "      <td>Electronic Funds Transfer PREAUTHORIZED DEBIT ...</td>\n",
       "      <td>342.74</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>Internet Banking INTERNET TRANSFER 000000289342</td>\n",
       "      <td>826.74</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>Internet Banking INTERNET TRANSFER 000000269477</td>\n",
       "      <td>236.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>Internet Banking INTERNET TRANSFER 000000252832</td>\n",
       "      <td>348.71</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>Internet Banking INTERNET TRANSFER 000000268499</td>\n",
       "      <td>623.09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>Internet Banking INTERNET DEPOSIT 000000277361</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1598.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>Internet Banking INTERNET BILL PAY 00000027078...</td>\n",
       "      <td>100.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          date                                               item    debit  \\\n",
       "0   2019-02-01  Electronic Funds Transfer PREAUTHORIZED DEBIT ...     0.04   \n",
       "1   2019-01-31         Branch Transaction SERVICE CHARGE DISCOUNT      NaN   \n",
       "2   2019-01-31                  Branch Transaction SERVICE CHARGE     7.45   \n",
       "3   2019-01-25  Internet Banking INTERNET BILL PAY 00000021643...   433.00   \n",
       "4   2019-01-25   Electronic Funds Transfer PAY DTCG DELOITTE INC.      NaN   \n",
       "5   2019-01-23  Electronic Funds Transfer PREAUTHORIZED DEBIT ...  3258.39   \n",
       "6   2019-01-21  Electronic Funds Transfer PREAUTHORIZED DEBIT ...    35.37   \n",
       "7   2019-01-21  Electronic Funds Transfer PREAUTHORIZED DEBIT ...    71.64   \n",
       "8   2019-01-21  Point of Sale - Interac RETAIL PURCHASE 902012...    27.73   \n",
       "9   2019-01-17  Internet Banking E-TRANSFER 101253831721 Hedie...   225.00   \n",
       "10  2019-01-16  Electronic Funds Transfer PREAUTHORIZED DEBIT ...   630.22   \n",
       "11  2019-01-16  Point of Sale - Interac RETAIL PURCHASE 000068...     0.78   \n",
       "12  2019-01-15  Electronic Funds Transfer DEPOSIT 0000759492 D...      NaN   \n",
       "13  2019-01-11   Electronic Funds Transfer PAY DTCG DELOITTE INC.      NaN   \n",
       "14  2019-01-07  Electronic Funds Transfer SAFETY DEPOSIT BOX R...    84.75   \n",
       "15  2019-01-07  Electronic Funds Transfer PREAUTHORIZED DEBIT ...   342.74   \n",
       "16  2019-01-03    Internet Banking INTERNET TRANSFER 000000289342   826.74   \n",
       "17  2019-01-03    Internet Banking INTERNET TRANSFER 000000269477   236.00   \n",
       "18  2019-01-03    Internet Banking INTERNET TRANSFER 000000252832   348.71   \n",
       "19  2019-01-03    Internet Banking INTERNET TRANSFER 000000268499   623.09   \n",
       "20  2019-01-03     Internet Banking INTERNET DEPOSIT 000000277361      NaN   \n",
       "21  2019-01-02  Internet Banking INTERNET BILL PAY 00000027078...   100.00   \n",
       "\n",
       "     credit  card  \n",
       "0       NaN   NaN  \n",
       "1      7.45   NaN  \n",
       "2       NaN   NaN  \n",
       "3       NaN   NaN  \n",
       "4   2634.50   NaN  \n",
       "5       NaN   NaN  \n",
       "6       NaN   NaN  \n",
       "7       NaN   NaN  \n",
       "8       NaN   NaN  \n",
       "9       NaN   NaN  \n",
       "10      NaN   NaN  \n",
       "11      NaN   NaN  \n",
       "12   951.51   NaN  \n",
       "13  2634.49   NaN  \n",
       "14      NaN   NaN  \n",
       "15      NaN   NaN  \n",
       "16      NaN   NaN  \n",
       "17      NaN   NaN  \n",
       "18      NaN   NaN  \n",
       "19      NaN   NaN  \n",
       "20  1598.00   NaN  \n",
       "21      NaN   NaN  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files[0]['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resetToCurrentData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1076,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./processed/processed.csv'"
      ]
     },
     "execution_count": 1076,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reset data and process from backup\n",
    "from shutil import copyfile\n",
    "copyfile(\"./backup/data.csv\", \"./processed/data.csv\")\n",
    "copyfile(\"./backup/processed.csv\", \"./processed/processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1020,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def changeData(hash, subCategory):\n",
    "    df = getFile(\"data\")\n",
    "    df['subCategory'].fillna(\"\",inplace=True)\n",
    "    df.fillna(value=0,inplace=True)\n",
    "    df.loc[dz[\"hash\"] == hash, \"subCategory\"] = subCategory\n",
    "    return df\n",
    "\n",
    "df = changeData(\"bec633716240d8202d8c6e815215fd20cd190bbf\", 'abbas')\n",
    "writeFile(\"data\", df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# HASHING UTILITIES\n",
    "# hash the data file\n",
    "# old = pd.read_csv(\"./processed/data.csv\", header=None,names=oldColumns)\n",
    "# old['custom'].fillna(\"\",inplace=True)\n",
    "# old.fillna(value=0,inplace=True)\n",
    "# assert old.dtypes['debit'] == 'float64'\n",
    "# assert old.dtypes['credit'] == 'float64'\n",
    "# old['hash']= hashit(old)\n",
    "# old.to_csv(f'./processed/data.csv', index=False, header=False)\n",
    "\n",
    "# hash the other files\n",
    "# for file in dlFiles:\n",
    "#     old = pd.read_csv(\"./processed/cards/\"+file, header=None,names=oldColumns)\n",
    "#     old['custom'].fillna(\"\",inplace=True)\n",
    "#     old.fillna(value=0,inplace=True)\n",
    "#     assert old.dtypes['debit'] == 'float64'\n",
    "#     assert old.dtypes['credit'] == 'float64'\n",
    "#     old['hash']= hashit(old)\n",
    "#     old.to_csv(f'./processed/{file.split(\".\")[0]}.{file.split(\".\")[1]}', index=False, header=False)\n",
    "\n",
    "# # populate account field in data\n",
    "# oldColumns=['date','item','debit','credit','custom','hash', 'account']\n",
    "\n",
    "# data = pd.read_csv(\"./processed/data.csv\", header=None,names=oldColumns)\n",
    "# for file in dlFiles:\n",
    "#     new = pd.read_csv(\"./processed/\"+file, header=None,names=oldColumns)\n",
    "#     for index, row in new.iterrows():\n",
    "#         if(row['hash'] in data['hash'].values):\n",
    "#             data.loc[data['hash'] == row['hash'],'account']=file.split(\".\")[0]\n",
    "            \n",
    "# data.to_csv(f'./processed/data.csv', index=False, header=False) \n",
    "\n",
    "\n",
    "# # check for duplicate fields\n",
    "\n",
    "# data2 = pd.read_csv(\"./processed/data.csv\", header=None,names=oldColumns)\n",
    "# for index, row in data2.iterrows():\n",
    "#     if(data2[data2['hash']==row['hash']].count()['hash']>2):\n",
    "#         print(row['hash'], row['item'], row['debit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # PROCESS BENCH MARKING\n",
    "# # TESTING TWO FILES\n",
    "# old = pd.read_csv(\"./processed/output-old.csv\", index_col=False)\n",
    "# new = pd.read_csv(\"./processed/processed.csv\", index_col=False)\n",
    "# old.sort_values(by=['date','debit'], inplace=True)\n",
    "# new.sort_values(by=['date','debit'], inplace=True)\n",
    "# old.reset_index( drop=True,inplace=True)\n",
    "# new.reset_index(drop=True,inplace=True)\n",
    "# ineq = []\n",
    "# for i, row in old.iterrows():\n",
    "#     if (row['subCategory'] != new.loc[i]['subCategory']):\n",
    "#         if(row['item']==\"TIGERDIRECT.CA MARKHAM, ON\"):\n",
    "#             pass\n",
    "#         elif('WINNERSHOMESENSE' in row['item']):\n",
    "#             pass\n",
    "#         else:\n",
    "#             pass\n",
    "# #             print(f\"{row['subCategory']}, {new.loc[i]['subCategory']} - {row['balance']} {row['item']}{new.loc[i]['item']}\")\n",
    "            \n",
    "\n",
    "# y = new.groupby('subCategory').sum()['balance']\n",
    "# x = old.groupby('subCategory').sum()['balance']\n",
    "# for i,value in enumerate(x):\n",
    "#     if(value != y[i]):\n",
    "#         print(value, y[i])\n",
    "\n",
    "# # one to one map benchmarking\n",
    "# def try1():    \n",
    "#     def check1to1(x):\n",
    "#         try:\n",
    "#             index = pd.Index(maps['item']).get_loc(x.rstrip())   \n",
    "#             return maps.loc[index]['subCategory']\n",
    "#         except:\n",
    "#             return None\n",
    "\n",
    "#     data['subCategory'] = data['subCategory'].combine_first(data['item'].apply(check1to1))\n",
    "    \n",
    "# def try2():\n",
    "#     data.fillna(\"\", inplace=True)\n",
    "#     subCatArray = []\n",
    "#     # first mapping the 1to1 mappings\n",
    "#     for i, row in data.iterrows():\n",
    "#         if (row['subCategory'] != \"\"):\n",
    "#             subCatArray.append(row['subCategory'])\n",
    "#         else:\n",
    "#             try:\n",
    "#                 index = pd.Index(maps['item']).get_loc(row['item'].rstrip())\n",
    "#                 subCategory = maps.loc[index]['subCategory']\n",
    "#                 subCatArray.append(subCategory)\n",
    "#             except:\n",
    "#                 subCatArray.append(\"\")\n",
    "#     data['subCategory'] = subCatArray\n",
    "# %timeit try1()\n",
    "# %timeit try2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
